
## Healthcare Data Pipeline Project
* Overview
> This project demonstrates an end-to-end data engineering pipeline for synthetic healthcare data. It covers data generation, ingestion, cleaning, transformation, storage, and basic analytics. The goal is to simulate a healthcare data workflow that includes patient records, diagnoses, and insurance claims, showcasing core data engineering skills with Python, Faker, and SQLite/PostgreSQL.

### Key features:

1. Synthetic healthcare data generation using Python Faker
2. Data cleaning and transformation with Pandas
3. Loading data into a relational database (SQLite or PostgreSQL)
4. Simple analytics and visualization of patient diagnoses
5. Pipeline automation concepts (e.g., Airflow or cron scheduling)

### Data Architecture
The pipeline follows a layered approach:

- Raw Data: Synthetic CSV files generated by Faker simulating patient and visit records.
- Silver Layer: Cleaned and standardized data after validation and transformation.
- Gold Layer: Structured tables in a relational database for querying and analytics.

This architecture allows modular and scalable data processing, similar to real-world healthcare data workflows.

##### Prerequisites
- Python 3.6 or higher
- Libraries: faker, pandas, sqlite3 (built-in), or psycopg2 for PostgreSQL
- Optional: PostgreSQL server if using PostgreSQL instead of SQLite
- Optional: Apache Airflow or cron for pipeline automation

*How to Run This Project*
1. Install required Python packages:

```bash

pip install faker pandas psycopg2-binary

```
2. Generate synthetic healthcare data:
3. Run your data generation script (e.g., generate_data.py) to create CSV files.
4. Clean and transform data:
5. Use your ETL script (e.g., etl.py) to clean the data and prepare it for loading.
6. Load data into the database:
7. Run the database loading script to insert data into SQLite or PostgreSQL.
8. Run analytics and visualization:
9. Execute analysis scripts or Jupyter notebooks to generate insights and charts.

10. (Optional) Automate the pipeline:

- Set up Apache Airflow DAGs or cron jobs to schedule data generation and ETL tasks.

# >>Lessons Learned
>Importance of data quality checks and schema validation in healthcare data.
>Challenges of simulating realistic healthcare datasets.
>Benefits of layered data architecture for scalable pipelines.
>Experience with Python data libraries and database interactions.
>Introduction to workflow automation tools like Airflow.

* Contact *🤙🏾
For questions or collaboration, please reach out:

LinkedIn: [LinkedIn](https://www.linkedin.com/in/michelle-wambaya/)

Email: michellewambaya@gmail.com
